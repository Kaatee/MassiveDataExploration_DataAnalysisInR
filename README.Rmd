---
title: "Eksploracja Masywnych Danych - projekt 1 - Analiza danych"
author: "Katarzyna Jóźwiak 127237, Piotr Pawlaczyk 127245"
output:
  #html_document: 
   #toc: true
   #theme: united
  md_document: 
   toc: true
---
data wygenerowania: '`r format(Sys.Date(), "%Y-%B-%d")`'

# Podsumowanie badań
***[TODO]*** podsumowanie calego raportu

# Wykorzystane biblioteki
Do analizy danych i stworzenia raportu z tej analizy zostały wykorzystane następujące biblioteki: 
- datasets    
- corrplot  
- ggplot2  
- gganimate  
- dplyr  
- gridExtra  
- gifski package  
- imputeTS   
- caret


# Zapewnienie powtarzalności wyników przy każdym uruchomieniu raportu na tych samych danych
Aby zapewnić powtarzalność próbkowań i losowań liczb (m. inn. przy próbkowaniu zbioru) przy każdym odpaleniu programu ustawiono stałe ziarno.  
```{r}
set.seed(23)
```

# Wstęp
W podanym sprawozdaniu użyto zbioru danych sledzie.csv pobranego ze strony http://www.cs.put.poznan.pl/alabijak/emd/projekt/sledzie.csv . 
Zbiór ten opisuje rozmiary śledzi i warunki, w których żyją od 60-ciu lat. Kolejne kolumny w zbiorze danych to:  
* **length**: długość złowionego śledzia [cm];  
* **cfin1**: dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 1];  
* **cfin2**: dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 2];  
* **chel1**: dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 1];  
* **chel2**: dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 2];  
* **lcop1**: dostępność planktonu [zagęszczenie widłonogów gat. 1];  
* **lcop2**: dostępność planktonu [zagęszczenie widłonogów gat. 2];  
* **fbar**: natężenie połowów w regionie [ułamek pozostawionego narybku];  
* **recr**: roczny narybek [liczba śledzi];  
* **cumf**: łączne roczne natężenie połowów w regionie [ułamek pozostawionego narybku];  
* **totaln**: łączna liczba ryb złowionych w ramach połowu [liczba śledzi];  
* **sst**: temperatura przy powierzchni wody [°C];  
* **sal**: poziom zasolenia wody [Knudsen ppt];  
* **xmonth**: miesiąc połowu [numer miesiąca];  
* **nao**: oscylacja północnoatlantycka [mb].  
Wiersze w zbiorze są uporządkowane chronologicznie.

# Wczytywanie danych z pliku
```{r}
names <- read.table("sledzie.csv", nrow=1, stringsAsFactors = FALSE, sep = ",")
data <- read.table("sledzie.csv", header=TRUE, stringsAsFactors = FALSE, sep=",")
head(data)
```
Jak widać, wczytane dane zawierają znak "?" przy nieznanych danych.

Wyświetlenie klas poszczególnych kolumn:
```{r}
sapply(data, class)
```
Dane zawierające znak "?" zostały zinterpretowane jako tekst.  
Z uwagi na występowanie brakujących danych konieczna była zmiana "?" na "NA" wraz ze zmianą typu danych z character na numeric:
```{r}
data[data=="?"] <- NA
```
Po zmianie znaku "?" na wartość NA zmianiono typ danych kolumn z nieznanymi wartościami na wartości numeryczne.  
```{r}
data$cfin1 <- as.numeric(data$cfin1)
data$cfin2 <- as.numeric(data$cfin2)
data$chel1 <- as.numeric(data$chel1)
data$chel2 <- as.numeric(data$chel2)
data$lcop1 <- as.numeric(data$lcop1)
data$lcop2 <- as.numeric(data$lcop2)
data$sst <- as.numeric(data$sst)

head(data)
sapply(data, class)

```

# Rozmiar zbioru danych i podstawowe statystyki
```{r}
paste("Wczytane dane zawierają ", nrow(data), " rekordów oraz ", ncol(data), " kolumn.", sep=" ")
```

Podsumowanie wartości danych:
```{r}
summary(data)
```

Ilość brakujących danych dla poszczególnych kolumn:
```{r}
missingData <- sapply(data, function(x) sum(is.na(x)))
missingData
```
Wykres ilości brakujących danych w zbiorze:

```{r}
plot(x = factor(names(missingData)), y = missingData, main="Wykres ilości brakujących danych w zbiorze", xlab="atrybut", ylab = "ilość brakujących artybutów")
```  

Jak widać zbiór danych zawiera znaczną ilość brakujących danych, co może utrudnić ich późniejszą analizę.

# Brakujące dane
```{r warning = FALSE, message = FALSE}
library("imputeTS")
data$cfin1 <- na_kalman(data$cfin1)
data$cfin2 <- na_kalman(data$cfin2)
data$chel1 <- na_kalman(data$chel1)
data$chel2 <- na_kalman(data$chel2)
data$lcop1 <- na_kalman(data$lcop1)
data$lcop2 <- na_kalman(data$lcop2)
data$sst <- na_kalman(data$sst)
```

Ilosć brakujących danych w poszczególnych kolumnach po zastosowaniu filtru Kalman'a:

```{r}
missingData <- sapply(data, function(x) sum(is.na(x)))
missingData
```

Z powyższej tabeli wynika, że filtr Kalman'a poradził sobie ze wszystkimi brakującymi danymi.


# Szczegółowa analiza zbiorów wartości
Poniżej przedstawiono rozkłady wszystkich wartośi w zbiorze danych  

```{r cache=TRUE, warning = FALSE, message = FALSE}
library(ggplot2)
library(ggExtra)

lengthDissPlot <- ggplot(data, aes(x=length)) + geom_histogram(binwidth=.5, colour="black", fill="white")
cfin1DissPlot <- ggplot(data, aes(x=cfin1)) + geom_histogram(binwidth=1, colour="black", fill="white")
cfin2DissPlot <- ggplot(data, aes(x=cfin2)) + geom_histogram(binwidth=1, colour="black", fill="white")
chel1DissPlot <- ggplot(data, aes(x=chel1)) + geom_histogram(binwidth=.5, colour="black", fill="white")
chel2DissPlot <- ggplot(data, aes(x=chel2)) + geom_histogram(binwidth=.5, colour="black", fill="white")
lcop1DissPlot <- ggplot(data, aes(x=lcop1)) + geom_histogram(binwidth=.5, colour="black", fill="white")
lcop2DissPlot <- ggplot(data, aes(x=lcop2)) + geom_histogram(binwidth=.5, colour="black", fill="white")
fbarDissPlot <- ggplot(data, aes(x=fbar)) + geom_histogram(binwidth=.05, colour="black", fill="white")
recrDissPlot <- ggplot(data, aes(x=recr)) + geom_histogram(binwidth=50000.0, colour="black", fill="white")
cumfDissPlot <- ggplot(data, aes(x=cumf)) + geom_histogram(binwidth=.02, colour="black", fill="white")
totalnDissPlot <- ggplot(data, aes(x=totaln)) + geom_histogram(binwidth=1000.0, colour="black", fill="white")
sstDissPlot <- ggplot(data, aes(x=sst)) + geom_histogram(binwidth=1.0, colour="black", fill="white")
salDissPlot <- ggplot(data, aes(x=sal)) + geom_histogram(binwidth=.01, colour="black", fill="white")
xmonthDissPlot <- ggplot(data, aes(x=xmonth)) + geom_histogram(binwidth=1.0, colour="black", fill="white")
naoDissPlot <- ggplot(data, aes(x=nao)) + geom_histogram(binwidth=.5, colour="black", fill="white")


require(gridExtra)
grid.arrange(lengthDissPlot,cfin1DissPlot,cfin2DissPlot ,chel1DissPlot ,chel2DissPlot,lcop1DissPlot,lcop2DissPlot ,fbarDissPlot ,
recrDissPlot ,cumfDissPlot ,totalnDissPlot ,sstDissPlot ,salDissPlot ,xmonthDissPlot ,naoDissPlot, nrow = 5, ncol=3)
```

Z powyższych wykresów wynika, że jeśli chodzi o rozkład poszczególnych atrybutów to zbiór nie jest zrównoważony.  

Analizując długość śledzia (*length*) można zauważyć, że ten atrybut przyjmuje rozkład zbliżony do rozkładu normalnego. W zbiorze danych znajduje się najwięcej śledzi o długości równej 25-26cm. Ze wcześniejszej analizy podsumowującej dane wynika także, że minimalna długość śledzia w zbiorze to 19cm, a maksymalna to 32.5cm. Średnia długość 25.3cm, a mediana to 25,5.  

Jeśli chodzi o atrybut jakim jest dostępność planktonu (zarówno zagęszczenie *Calanus finmarchicus gat. 1* jak i *Calanus finmarchicus gat. 2*) to można zauważyć że w zbiorze znalazło się znacznie więcej danych z łowisk o mniejszej dostępności planktonu omawianego gatunku. Ze wcześniejszej analizy wiemy, że minimalna dostępność pierwszego gatunku tego planktonu to 0, maksymalna: ok 37,67. Srednia to ok. 0,45 a mediana to ok. 0,11. Jeśli chodzi o drugi gatunek planktonu *Calanus finmarchicus* to jego minimalne zagęszczenie wynosi 0, maksymalne - ok. 19,4, średnia to ok. 2,03 a mediana - 0.7.  

Kolejnym badanym atrybutem jest dostępność planktonu gatunku *Calanus helgolandicus gat. 1* i *Calanus helgolandicus gat. 1*. W przypadku pierwszego z nich w zbiorze znalazło się więcej śledzi złowionych na łowisku o mniejszej zawartości planktonu. W Przypadku drugiego gatunku planktonu rozkład ilości złowionych tam śledzi był bardziej rozłożony w przedziale 0-40. O śledziach złapanych w miejscach gdzie zagęszczenie omawianego planktonu drugiego gatunku osiąga przedział 40-60 można powiedzieć że są towartości odstające. Jeśli chodzi o zagęszczenie pierwszego gatunku to ze wcześniejszej analizy wynika, że atrubut tenprzyjmuje minimalną wartość równą 0, a maksymalną równą 75. Jego średnia występowania to ok. 10, a mediana to 5.75. Jeśli chodzi o minimalną wartość dostępności planktonu drugiego gatunku *Calanus helgolandicus* to jego minimalna wartość to ok. 5.2, maksymalna: ok. 57.7, średnia to ok. 21.2, a mediana wynosi około 21.7.  

Ostatnim z planktonów, których dostępność badaliśmy są widłonogi dwóch gatunków (*lcop1* i *lcop2*). 
***[TOTO]***
Kolejnym atrybutem jest natężenie połowów w regionie (ułamek pozostawionego narybku) - *fbar*.
***[TOTO]***
Następnym z znalizowanych atrybutów będzie ilość rocznego narybku (czyli liczba śledzi) - *recr*.
***[TOTO]***
***cumf: łączne roczne natężenie połowów w regionie [ułamek pozostawionego narybku];
* totaln: łączna liczba ryb złowionych w ramach połowu [liczba śledzi];
* sst: temperatura przy powierzchni wody [°C];
* sal: poziom zasolenia wody [Knudsen ppt];
* xmonth: miesiąc połowu [numer miesiąca];
* nao: oscylacja północnoatlantycka [mb].***


# Korelacja między zmiennymi

W niniejszej sekcji została przeanalizowana korelacja między atrybutami.Na początek przedstawiono współczynniki korelacji wraz z graficzną macierzą korelacji.  

Największa korelacja występuje między atrybutem *chel1* i *lcop1* - czyli między dostępnością planktonu dwóch gatunków i wynosi ona 0.96. Między *chel2* i *lcop2* - współczynnik korelacji wynosi 0.89, a między *cumf* ( łączne roczne natężenie połowów w regionie) a *fbar* (natężenie połowów w regionie): 0.82 (zależność między tymi ostatnimi wynika prawdopodobnie z tego, że na podstawie jednego z tych atrybutów w naturalny sposób oblicza się dtugi z nich). Wysoka korelacja występuje także między *cfin2* a *lcop2*, gdzie współczynnik korelacji jest równy 0.65.  

Bardzo niski, jednak ujemny współczynnik korelacji (wynoszący -0.55) miedzy atrybutem *lcop1* a *neo* (oscylacja północnoatlantycka) świadczy o wysokiej, ale odwrotnej zaleznosci tych atrybutów. Może to być wyjaśnione z punktu widzenia przyrody przez to, że plankton może być przenoszony przez prądy morskie.  

Niski ujemny współczynnik korelacji wskazujący na wysoką korelacje atrybutów można też zauważyć między *cumf* (łączne roczne natężenie połowów w regionie) a *totaln* (łączna liczba ryb złowionych w ramach połowu) - wynosi on -0.71, gdzie taka zależność też jest naturalna.  

Jeśli chodzi o atrybut, którego będą dotyczyć badania w dalszej części pracy, czyli ługość śledzia, to jest najbardziej skorelowana z temperaturą przy powierzchni wody, a współczynnik tej korelacji wynosi -0.45 (czyli są to wartości odwrotnie zależne)


```{r cache=TRUE, warning = FALSE, message = FALSE}
res <- cor(data)

library(corrplot)
corrplot.mixed(res, tl.col = "black", tl.srt = 45)
```

Żeby lepiej zwizualizować korelacje między omówionymi atrybutami przedstawiono wykresy zależności tych atrybutów.

```{r cache=TRUE}
plot(data[,c(5,7,16)])
```

W przypadku zaprezentowanych powyżej danych, zgodnie z wartościami mieszczącymi się w macierzy korelacji, największą korelacje (współczynnik korelacji wynoszący 0.96) widać między dostępnością planktonu pierwszego gatunku Calanus helgolandicus (*chel1*) a zagęszczeniem widłonogów z gatunku pierwszego (*lcop2*). Z wykresu wynika, że im więcej jednego plantonu w łowisku, tym spotkać tam można więcej drugiego z wymienionych planktonów.  
Zależność między *lcop2* a  oscylacja północnoatlantycka  *neo* nie jest aż tak widoczna na wykresie (choć też da się ją zauważyć).  


```{r cache=TRUE}
plot(data[,c(6,8,4)])
```

Podobnie jak na poprzednim wykresie, i tu najbardziej widoczna jest korelacja między dostępnością dwóch z gatunków planktonu.

```{r cache=TRUE}
plot(data[,c(9,11,12)])
```

Na powyższym wykresie korelacji najbardziej widać odwrotnie proporcjonalną zależność między *fbar* a *totaln* - współczynnik korelacjiwynoszący -0.71) i *cumf* a *totaln*. Można wyraźnie zauważyć że im więcej pierwszego z atrybutów, tym mniej drugiego i odwrotnie.  
Wysoce proporcjonalne są zmienne *fbar* i *cumf*.

# Zmiana rozmiaru śledzia w czasie

```{r, warning = FALSE, message = FALSE}
library(ggplot2)
library(gganimate)
library(dplyr)

sampled <- sample_n(data,  round(nrow(data)*0.1))

p <- ggplot(
  sampled, aes(X, length, group = xmonth, color = factor(xmonth))) +
  geom_line() +
  scale_color_viridis_d() +
  labs(x = "Kolejne połowy", y = "Dlugosc sledzia") +
  theme(legend.position = "top")

```

```{r}
p + transition_reveal(X)
```

Z wykresu można zauważyć delikatny spadek śledzia w czasie. Choć na początku rozmiar śledzia znaczie rośnie, to po którkim czasie zaczyna już maleć.

# Przewidywanie rozmiaru śledzia
***[TODO]*** wywaliś z danych X!!!
W niniejszej sekcji zostanie podjęta próba stworzenia regresora przewidującego rozmiar śledzia.  

W niniejszym rozdziale postaramy się przewidzieć rozmiar śledzia z wykorzystaniem różnych metod uczenia maszynowego. W tym celu na początek dane zostaną podzielone na zbiór uczący (stanowiący 75% całego zbioru danych) i testowy (stanowiący 25% całego zbioru danych). 

Parametry zostały domyślnie zoptymalizowane wybierając trzy wartości dla każdego zdefiniowanego dla modelu parametru optymalizacyjnego.  

Do badań zostanie wykorzystana powtarzana ocena krzyżowa.
```{r warning = FALSE, message = FALSE}
library(caret)
library(ggplot2)
library(gganimate)
library(dplyr)

inTraining <- createDataPartition(y = data$length, p = .75,list = FALSE)

training <- data[ inTraining, ]
testing  <- data[-inTraining, ]

ctrl <- trainControl( method = "repeatedcv", number = 5, repeats = 10)
```
## Random Forest
Pierwszym z wykorzystanych algorytmów będzie Random Forest z liczbą drzew w lesie równą 10.

```{r cache=TRUE}
fitRandomForestF <- train(length ~ ., data = training, method = "rf", importance=T, trControl = ctrl, ntree = 10) 
fitRandomForestF

rfClasses <- predict(fitRandomForestF, newdata = testing)
rfClasses <- sapply(rfClasses, round, digits = 0)

availableValues <- sapply(testing$length, round, digits = 0)
levels <- unique(c(availableValues, rfClasses))

confusionMatrix <- confusionMatrix(data = factor(rfClasses, levels = levels),   factor(availableValues, levels = levels))
confusionMatrix
```

```{r}
confusionMatrix$overall
```

***[TODO]*** opisac cos o tym

## Regresja liniowa
Kolejnym przebadanym regresorem będzie regresja liniowa 

```{r cache=TRUE, warning = FALSE}
fitLR <- train(length ~ ., data = training, method = "lm", importance=T, trControl = ctrl)
fitLR

lrClasses <- predict(fitLR, newdata = testing)
lrClasses <- sapply(lrClasses, round, digits = 0)

availableValues <- sapply(testing$length, round, digits = 0)
levels <- unique(c(availableValues, lrClasses))

confusionMatrix <- confusionMatrix(data = factor(lrClasses, levels = levels),   factor(availableValues, levels = levels))
confusionMatrix
```

```{r}
confusionMatrix$overall
```

***[TODO]*** opisac cos o tym

## kNN
Ostatnim z przebadanych algorytmów został kNN  
```{r cache=TRUE, warning = FALSE}
fitKNN <- train(length ~ ., data = training, method = "knn", importance=T, trControl = ctrl)
fitKNN

knnClasses <- predict(fitKNN, newdata = testing)
knnClasses <- sapply(knnClasses, round, digits = 0)

availableValues <- sapply(testing$length, round, digits = 0)
levels <- unique(c(availableValues, knnClasses))

confusionMatrix <- confusionMatrix(data = factor(knnClasses, levels = levels),   factor(availableValues, levels = levels))
confusionMatrix
```

```{r}
plot(fitKNN)
confusionMatrix$overall
```

***[TODO]*** opisac cos o tym  

## Porównanie modeli
Na koniec modele zostały porównane na podstawie miary średniej kwadratowej błędów RMSE (w przypadku tej miary, im mniejszą wartość osiągnie algorytm, tym jest lepszy) i współczynnik determinacji R^2^ (dopsowanie modelu jest tym lepsze im wartość R^2^ jest bliższa jedności):  

```{r}
x <- list(randomForest = fitRandomForestF, linearRegression = fitLR, kNN = fitKNN) %>% resamples()

dotplot(x, metric = "RMSE")
dotplot(x, metric = "Rsquared")
```

# Analiza ważnośći atrybutów najlepszego znalezionego modelu regresji  
***[TODO]*** Analiza ważności atrybutów najlepszego znalezionego modelu regresji. Analiza ważności atrybutów powinna stanowić próbę odpowiedzi na pytanie: co sprawia, że rozmiar śledzi zaczął w pewnym momencie maleć.  

Najlepszym znalezionym modelem regresji okazał się być algorytm kNN, dla którego przeanalizowaliśmy ważność atrybutów:

```{r}
ggplot(varImp(fitKNN))
```
***[TODO]*** opisac
